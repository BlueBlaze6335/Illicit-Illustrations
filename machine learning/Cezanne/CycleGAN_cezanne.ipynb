{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Introduction\nIn this notebook , we are going to Generate Art using GANS. Explaining it in a brief way , it is a Artist who forges Ukiyoe paintings , but using random normal picture that is assigned to him. YES you can have your own portrait made in Ukiyoe style!!! However , this Artist has both his ears","metadata":{}},{"cell_type":"code","source":"# %tensorflow_version 2.x\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))\n\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     print('Device:', tpu.master())\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# except:\n#     strategy = tf.distribute.get_strategy()\n# print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:52:02.52724Z","iopub.execute_input":"2022-03-06T19:52:02.527568Z","iopub.status.idle":"2022-03-06T19:52:08.700914Z","shell.execute_reply.started":"2022-03-06T19:52:02.527492Z","shell.execute_reply":"2022-03-06T19:52:08.698391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install git+https://www.github.com/keras-team/keras-contrib.git","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:52:08.702719Z","iopub.execute_input":"2022-03-06T19:52:08.702987Z","iopub.status.idle":"2022-03-06T19:52:08.707069Z","shell.execute_reply.started":"2022-03-06T19:52:08.70295Z","shell.execute_reply":"2022-03-06T19:52:08.706312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_datasets as tfds\n \nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom tensorflow.keras.initializers import RandomNormal\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Conv2DTranspose\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow import keras\n#from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom tensorflow.keras.models import Model\n\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:52:08.708434Z","iopub.execute_input":"2022-03-06T19:52:08.708893Z","iopub.status.idle":"2022-03-06T19:52:10.580406Z","shell.execute_reply.started":"2022-03-06T19:52:08.708852Z","shell.execute_reply":"2022-03-06T19:52:10.579713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading the Data:\nWe are using the cezanne2photo data from TensorFlow Datasets , from the cycle_gan category. However we are inverting the data , that is photo to cezanne.","metadata":{}},{"cell_type":"code","source":"dataset, metadata = tfds.load('cycle_gan/cezanne2photo',\n                              with_info=True, as_supervised=True,try_gcs=True)\n\ntrain_normal, train_van = dataset['trainB'], dataset['trainA']\ntest_normal, test_van = dataset['testB'], dataset['testA']","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:52:10.582358Z","iopub.execute_input":"2022-03-06T19:52:10.582605Z","iopub.status.idle":"2022-03-06T19:53:08.335027Z","shell.execute_reply.started":"2022-03-06T19:52:10.582569Z","shell.execute_reply":"2022-03-06T19:53:08.334294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 1000\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_SHAPE = (256,256,3)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:08.336224Z","iopub.execute_input":"2022-03-06T19:53:08.33658Z","iopub.status.idle":"2022-03-06T19:53:08.34442Z","shell.execute_reply.started":"2022-03-06T19:53:08.336535Z","shell.execute_reply":"2022-03-06T19:53:08.3437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_crop(image):\n  cropped_image = tf.image.random_crop(\n      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:08.345821Z","iopub.execute_input":"2022-03-06T19:53:08.34728Z","iopub.status.idle":"2022-03-06T19:53:08.587175Z","shell.execute_reply.started":"2022-03-06T19:53:08.347235Z","shell.execute_reply":"2022-03-06T19:53:08.586319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize(image):\n  image = tf.cast(image, tf.float32)\n  image = (image / 127.5) -1\n  return image","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:08.58823Z","iopub.execute_input":"2022-03-06T19:53:08.589085Z","iopub.status.idle":"2022-03-06T19:53:08.596083Z","shell.execute_reply.started":"2022-03-06T19:53:08.589045Z","shell.execute_reply":"2022-03-06T19:53:08.594122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_jitter(image):\n  # resizing to 286 x 286 x 3\n  image = tf.image.resize(image, [286, 286],\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  # randomly cropping to 256 x 256 x 3\n  image = random_crop(image)\n\n  # random mirroring\n  image = tf.image.random_flip_left_right(image)\n\n  return image","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:08.59715Z","iopub.execute_input":"2022-03-06T19:53:08.597372Z","iopub.status.idle":"2022-03-06T19:53:08.603885Z","shell.execute_reply.started":"2022-03-06T19:53:08.597347Z","shell.execute_reply":"2022-03-06T19:53:08.603074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image_train(image, label):\n  image = random_jitter(image)\n  image = normalize(image)\n  return image","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:08.605144Z","iopub.execute_input":"2022-03-06T19:53:08.605713Z","iopub.status.idle":"2022-03-06T19:53:08.612482Z","shell.execute_reply.started":"2022-03-06T19:53:08.605673Z","shell.execute_reply":"2022-03-06T19:53:08.61164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image_test(image, label):\n  image = random_jitter(image)\n  image = normalize(image)\n  return image","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:08.61597Z","iopub.execute_input":"2022-03-06T19:53:08.617019Z","iopub.status.idle":"2022-03-06T19:53:08.62142Z","shell.execute_reply.started":"2022-03-06T19:53:08.616979Z","shell.execute_reply":"2022-03-06T19:53:08.620568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_normal = train_normal.map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n    BUFFER_SIZE).batch(1)\n\ntrain_van = train_van.map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n    BUFFER_SIZE).batch(1)\n\ntest_normal = test_normal.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).shuffle(\n    BUFFER_SIZE).batch(1)\n\ntest_van = test_van.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).shuffle(\n    BUFFER_SIZE).batch(1)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:08.623021Z","iopub.execute_input":"2022-03-06T19:53:08.623431Z","iopub.status.idle":"2022-03-06T19:53:08.915142Z","shell.execute_reply.started":"2022-03-06T19:53:08.623396Z","shell.execute_reply":"2022-03-06T19:53:08.914447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_normal = next(iter(train_normal))\nsample_van = next(iter(train_van))","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:08.916488Z","iopub.execute_input":"2022-03-06T19:53:08.916743Z","iopub.status.idle":"2022-03-06T19:53:14.258824Z","shell.execute_reply.started":"2022-03-06T19:53:08.916708Z","shell.execute_reply":"2022-03-06T19:53:14.258011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(121)\nplt.title('Normal pic')\nplt.imshow(sample_normal[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Normal pic with random jitter')\nplt.imshow(random_jitter(sample_normal[0]) * 0.5 + 0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:14.260507Z","iopub.execute_input":"2022-03-06T19:53:14.260775Z","iopub.status.idle":"2022-03-06T19:53:14.966355Z","shell.execute_reply.started":"2022-03-06T19:53:14.260739Z","shell.execute_reply":"2022-03-06T19:53:14.965285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(121)\nplt.title('cezanne Painting')\nplt.imshow(sample_van[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('cezanne Painting with random jitter')\nplt.imshow(random_jitter(sample_van[0]) * 0.5 + 0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:14.967677Z","iopub.execute_input":"2022-03-06T19:53:14.967951Z","iopub.status.idle":"2022-03-06T19:53:15.291881Z","shell.execute_reply.started":"2022-03-06T19:53:14.967913Z","shell.execute_reply":"2022-03-06T19:53:15.291228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting the Models:\nWe are implementing CycleGan , with a modified UNET model as a generator , and a PatchGan block as a Discriminator. The difference between a normal block and a GAN block is that it uses a InstanceNormalization..","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:15.292907Z","iopub.execute_input":"2022-03-06T19:53:15.293237Z","iopub.status.idle":"2022-03-06T19:53:15.462154Z","shell.execute_reply.started":"2022-03-06T19:53:15.293207Z","shell.execute_reply":"2022-03-06T19:53:15.461395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def downsample(filters, size, norm_type='batchnorm', apply_norm=True):\n  \"\"\"Downsamples an input.\n  Conv2D => Batchnorm => LeakyRelu\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n    apply_norm: If True, adds the batchnorm layer\n  Returns:\n    Downsample Sequential Model\n  \"\"\"\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n  if apply_norm:\n    if norm_type.lower() == 'batchnorm':\n      result.add(tf.keras.layers.BatchNormalization())\n    elif norm_type.lower() == 'instancenorm':\n      result.add(InstanceNormalization())\n\n  result.add(tf.keras.layers.LeakyReLU())\n\n  return result","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:15.463449Z","iopub.execute_input":"2022-03-06T19:53:15.465078Z","iopub.status.idle":"2022-03-06T19:53:15.496319Z","shell.execute_reply.started":"2022-03-06T19:53:15.465041Z","shell.execute_reply":"2022-03-06T19:53:15.495701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"down_model = downsample(3, 4)\ndown_result = down_model(tf.expand_dims(sample_van[0], 0))\nprint (down_result.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:15.497532Z","iopub.execute_input":"2022-03-06T19:53:15.498252Z","iopub.status.idle":"2022-03-06T19:53:21.664803Z","shell.execute_reply.started":"2022-03-06T19:53:15.498215Z","shell.execute_reply":"2022-03-06T19:53:21.664044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n  \"\"\"Upsamples an input.\n  Conv2DTranspose => Batchnorm => Dropout => Relu\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n    apply_dropout: If True, adds the dropout layer\n  Returns:\n    Upsample Sequential Model\n  \"\"\"\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n  if norm_type.lower() == 'batchnorm':\n    result.add(tf.keras.layers.BatchNormalization())\n  elif norm_type.lower() == 'instancenorm':\n    result.add(InstanceNormalization())\n\n  if apply_dropout:\n    result.add(tf.keras.layers.Dropout(0.5))\n\n  result.add(tf.keras.layers.ReLU())\n\n  return result","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:21.665941Z","iopub.execute_input":"2022-03-06T19:53:21.666711Z","iopub.status.idle":"2022-03-06T19:53:21.675004Z","shell.execute_reply.started":"2022-03-06T19:53:21.666674Z","shell.execute_reply":"2022-03-06T19:53:21.673706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"up_model = upsample(3, 4)\nup_result = up_model(down_result)\nprint (up_result.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:21.67602Z","iopub.execute_input":"2022-03-06T19:53:21.676455Z","iopub.status.idle":"2022-03-06T19:53:22.171096Z","shell.execute_reply.started":"2022-03-06T19:53:21.676415Z","shell.execute_reply":"2022-03-06T19:53:22.170342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Generator :\nThere are 2 generators (A and B) and 2 discriminators (X and Y) being trained here.\n\nGenerator G learns to transform image X to image Y. (A:X−>Y) Generator F learns to transform image Y to image X. (B:Y−>X) Discriminator D_X learns to differentiate between image X and generated image X (B(Y)). Discriminator D_Y learns to differentiate between image Y and generated image Y (A(X)).","metadata":{}},{"cell_type":"code","source":"def resnet_block(n_filters, input_layer):\n\t# weight initialization\n\tinit = RandomNormal(stddev=0.02)\n\t# first layer convolutional layer\n\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n\tg = tf.keras.layers.BatchNormalization()(g)\n\tg = Activation('relu')(g)\n\t# second convolutional layer\n\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n\tg = tf.keras.layers.BatchNormalization()(g)\n\t# concatenate merge channel-wise with input layer\n\tg = Concatenate()([g, input_layer])\n\treturn g","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:22.172366Z","iopub.execute_input":"2022-03-06T19:53:22.172616Z","iopub.status.idle":"2022-03-06T19:53:22.181418Z","shell.execute_reply.started":"2022-03-06T19:53:22.172585Z","shell.execute_reply":"2022-03-06T19:53:22.180699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def define_generator(image_shape, n_resnet=4):\n\t# weight initialization\n\tinit = RandomNormal(stddev=0.02)\n\t# image input\n\tin_image = Input(shape=image_shape)\n\t# c7s1-64\n\tg = Conv2D(32, (8,8), padding='same', kernel_initializer=init)(in_image)\n\tg =tf.keras.layers.BatchNormalization()(g)\n\tg = Activation('relu')(g)\n\t# d128\n\tg = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = tf.keras.layers.BatchNormalization()(g)\n\tg = Activation('relu')(g)\n\t# d256\n\tg = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg =tf.keras.layers.BatchNormalization()(g)\n\tg = Activation('relu')(g)\n \n # d512\n# \tg = Conv2D(256, (2,2), strides=(2,2), padding='same', kernel_initializer=init)(g)\n# \tg =tf.keras.layers.BatchNormalization()(g)\n# \tg = Activation('relu')(g)\n\t# R512\n\tfor _ in range(n_resnet):\n\t\tg = resnet_block(128, g)\n\t# \t# u256\n\t# \tg = Conv2DTranspose(256, (5,5), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\t# g = tf.keras.layers.BatchNormalization()(g)\n\t# g = Activation('relu')(g)\n\t# u128\n\tg = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = tf.keras.layers.BatchNormalization()(g)\n\tg = Activation('relu')(g)\n\t# u64\n\tg = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = tf.keras.layers.BatchNormalization()(g)\n\tg = Activation('relu')(g)\n \t# u64\n# \tg = Conv2DTranspose(32, (2,2), strides=(2,2), padding='same', kernel_initializer=init)(g)\n# \tg = tf.keras.layers.BatchNormalization()(g)\n# \tg = Activation('relu')(g)\n\t# c7s1-3\n\tg = Conv2D(3, (8,8), padding='same', kernel_initializer=init)(g)\n\tg = tf.keras.layers.BatchNormalization()(g)\n\tout_image = Activation('tanh')(g)\n\t# define model\n\tmodel = Model(in_image, out_image)\n\treturn model","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:22.184477Z","iopub.execute_input":"2022-03-06T19:53:22.184819Z","iopub.status.idle":"2022-03-06T19:53:22.196726Z","shell.execute_reply.started":"2022-03-06T19:53:22.184787Z","shell.execute_reply":"2022-03-06T19:53:22.195972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = define_generator(IMG_SHAPE)\ntf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)\ngenerator.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:22.19795Z","iopub.execute_input":"2022-03-06T19:53:22.198515Z","iopub.status.idle":"2022-03-06T19:53:23.474439Z","shell.execute_reply.started":"2022-03-06T19:53:22.198479Z","shell.execute_reply":"2022-03-06T19:53:23.473605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef discriminator(norm_type='batchnorm', target=True):\n  \"\"\"PatchGan discriminator model (https://arxiv.org/abs/1611.07004).\n  Args:\n    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n    target: Bool, indicating whether target image is an input or not.\n  Returns:\n    Discriminator model\n  \"\"\"\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')\n  x = inp\n\n  if target:\n    tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n    x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n\n  down1 = downsample(64, 4, norm_type, False)(x)  # (bs, 128, 128, 64)\n  down2 = downsample(128, 4, norm_type)(down1)  # (bs, 64, 64, 128)\n  down3 = downsample(256, 4, norm_type)(down2)  # (bs, 32, 32, 256)\n\n  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n  conv = tf.keras.layers.Conv2D(\n      512, 4, strides=1, kernel_initializer=initializer,\n      use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n\n  if norm_type.lower() == 'batchnorm':\n    norm1 = tf.keras.layers.BatchNormalization()(conv)\n  elif norm_type.lower() == 'instancenorm':\n    norm1 = InstanceNormalization(axis=-1)(conv)\n\n  leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n\n  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n\n  last = tf.keras.layers.Conv2D(\n      1, 4, strides=1,\n      kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n\n  if target:\n    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n  else:\n    return tf.keras.Model(inputs=inp, outputs=last)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:23.476117Z","iopub.execute_input":"2022-03-06T19:53:23.476387Z","iopub.status.idle":"2022-03-06T19:53:23.489557Z","shell.execute_reply.started":"2022-03-06T19:53:23.47635Z","shell.execute_reply":"2022-03-06T19:53:23.488453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_a = define_generator(IMG_SHAPE)\ngenerator_b = define_generator(IMG_SHAPE)\n\ndiscriminator_x = discriminator(norm_type='batchnorm', target=False)\ndiscriminator_y = discriminator(norm_type='batchnorm', target=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:23.491032Z","iopub.execute_input":"2022-03-06T19:53:23.491303Z","iopub.status.idle":"2022-03-06T19:53:24.298248Z","shell.execute_reply.started":"2022-03-06T19:53:23.49127Z","shell.execute_reply":"2022-03-06T19:53:24.297509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_van = generator_a(sample_normal)\nto_normal = generator_b(sample_van)\nplt.figure(figsize=(8, 8))\ncontrast = 8\n\nimgs = [sample_normal, to_van, sample_van, to_normal]\ntitle = ['Normal', 'To Ukiyoe', 'Ukiyoe', 'To Normal']\n\nfor i in range(len(imgs)):\n  plt.subplot(2, 2, i+1)\n  plt.title(title[i])\n  if i % 2 == 0:\n    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n  else:\n    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:24.299724Z","iopub.execute_input":"2022-03-06T19:53:24.299974Z","iopub.status.idle":"2022-03-06T19:53:25.097021Z","shell.execute_reply.started":"2022-03-06T19:53:24.299939Z","shell.execute_reply":"2022-03-06T19:53:25.096238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\n\nplt.subplot(121)\nplt.title('Is a real Ukiyoe Painting?')\nplt.imshow(discriminator_y(sample_van)[0, ..., -1], cmap='RdBu_r')\n\nplt.subplot(122)\nplt.title('Is a real Normal Pic?')\nplt.imshow(discriminator_x(sample_normal)[0, ..., -1], cmap='RdBu_r')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.098266Z","iopub.execute_input":"2022-03-06T19:53:25.098797Z","iopub.status.idle":"2022-03-06T19:53:25.406206Z","shell.execute_reply.started":"2022-03-06T19:53:25.09876Z","shell.execute_reply":"2022-03-06T19:53:25.40548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LAMBDA = 8","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.407269Z","iopub.execute_input":"2022-03-06T19:53:25.407495Z","iopub.status.idle":"2022-03-06T19:53:25.411428Z","shell.execute_reply.started":"2022-03-06T19:53:25.407461Z","shell.execute_reply":"2022-03-06T19:53:25.410692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.416376Z","iopub.execute_input":"2022-03-06T19:53:25.416732Z","iopub.status.idle":"2022-03-06T19:53:25.420997Z","shell.execute_reply.started":"2022-03-06T19:53:25.416697Z","shell.execute_reply":"2022-03-06T19:53:25.420322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(real, generated):\n  real_loss = loss_obj(tf.ones_like(real), real)\n\n  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss * 0.5","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.422264Z","iopub.execute_input":"2022-03-06T19:53:25.422554Z","iopub.status.idle":"2022-03-06T19:53:25.429242Z","shell.execute_reply.started":"2022-03-06T19:53:25.422518Z","shell.execute_reply":"2022-03-06T19:53:25.428533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generator_loss(generated):\n  return loss_obj(tf.ones_like(generated), generated)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.430383Z","iopub.execute_input":"2022-03-06T19:53:25.430872Z","iopub.status.idle":"2022-03-06T19:53:25.437698Z","shell.execute_reply.started":"2022-03-06T19:53:25.430837Z","shell.execute_reply":"2022-03-06T19:53:25.436979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_cycle_loss(real_image, cycled_image):\n  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n  \n  return LAMBDA * loss1","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.439077Z","iopub.execute_input":"2022-03-06T19:53:25.439392Z","iopub.status.idle":"2022-03-06T19:53:25.445679Z","shell.execute_reply.started":"2022-03-06T19:53:25.439359Z","shell.execute_reply":"2022-03-06T19:53:25.445066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def identity_loss(real_image, same_image):\n  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n  return LAMBDA * 0.5 * loss","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.447386Z","iopub.execute_input":"2022-03-06T19:53:25.447619Z","iopub.status.idle":"2022-03-06T19:53:25.455254Z","shell.execute_reply.started":"2022-03-06T19:53:25.447595Z","shell.execute_reply":"2022-03-06T19:53:25.454519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_a_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.7)\ngenerator_b_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.7)\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.456631Z","iopub.execute_input":"2022-03-06T19:53:25.456888Z","iopub.status.idle":"2022-03-06T19:53:25.46518Z","shell.execute_reply.started":"2022-03-06T19:53:25.456856Z","shell.execute_reply":"2022-03-06T19:53:25.464213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkpoint_path = \"./checkpoints/train\"\n\n# ckpt = tf.train.Checkpoint(generator_a=generator_a,\n#                            generator_b=generator_b,\n#                            discriminator_x=discriminator_x,\n#                            discriminator_y=discriminator_y,\n#                            generator_a_optimizer=generator_a_optimizer,\n#                            generator_b_optimizer=generator_b_optimizer,\n#                            discriminator_x_optimizer=discriminator_x_optimizer,\n#                            discriminator_y_optimizer=discriminator_y_optimizer)\n\n# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# # if a checkpoint exists, restore the latest checkpoint.\n# if ckpt_manager.latest_checkpoint:\n#   ckpt.restore(ckpt_manager.latest_checkpoint)\n#   print ('Latest checkpoint restored!!')","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.466786Z","iopub.execute_input":"2022-03-06T19:53:25.467067Z","iopub.status.idle":"2022-03-06T19:53:25.473621Z","shell.execute_reply.started":"2022-03-06T19:53:25.467003Z","shell.execute_reply":"2022-03-06T19:53:25.472906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 200","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.474973Z","iopub.execute_input":"2022-03-06T19:53:25.475221Z","iopub.status.idle":"2022-03-06T19:53:25.484725Z","shell.execute_reply.started":"2022-03-06T19:53:25.475187Z","shell.execute_reply":"2022-03-06T19:53:25.484105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_images(model, test_input):\n  prediction = model(test_input)      \n  plt.figure(figsize=(12, 12))\n \n  display_list = [test_input[0], prediction[0]]\n  title = ['Input Image', 'Predicted Painting']\n \n \n  for i in range(2):\n    plt.subplot(1, 2, i+1)\n    plt.title(title[i])\n    # getting the pixel values between [0, 1] to plot it.\n    plt.imshow(display_list[i] * 0.5 + 0.5)\n    plt.axis('off')\n  plt.show()\n  return prediction","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.486107Z","iopub.execute_input":"2022-03-06T19:53:25.486605Z","iopub.status.idle":"2022-03-06T19:53:25.49427Z","shell.execute_reply.started":"2022-03-06T19:53:25.486569Z","shell.execute_reply":"2022-03-06T19:53:25.493509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(real_x, real_y):\n  # persistent is set to True because the tape is used more than\n  # once to calculate the gradients.\n  with tf.GradientTape(persistent=True) as tape:\n    # Generator G translates X -> Y\n    # Generator F translates Y -> X.\n    \n    fake_y = generator_a(real_x, training=True)\n    cycled_x = generator_b(fake_y, training=True)\n\n    fake_x = generator_b(real_y, training=True)\n    cycled_y = generator_a(fake_x, training=True)\n\n    # same_x and same_y are used for identity loss.\n    same_x = generator_b(real_x, training=True)\n    same_y = generator_a(real_y, training=True)\n\n    disc_real_x = discriminator_x(real_x, training=True)\n    disc_real_y = discriminator_y(real_y, training=True)\n\n    disc_fake_x = discriminator_x(fake_x, training=True)\n    disc_fake_y = discriminator_y(fake_y, training=True)\n\n    # calculate the loss\n    gen_a_loss = generator_loss(disc_fake_y)\n    gen_b_loss = generator_loss(disc_fake_x)\n    \n    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n    \n    # Total generator loss = adversarial loss + cycle loss\n    total_gen_a_loss = gen_a_loss + total_cycle_loss + identity_loss(real_y, same_y)\n    total_gen_b_loss = gen_b_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n  \n  # Calculate the gradients for generator and discriminator\n  generator_a_gradients = tape.gradient(total_gen_a_loss, \n                                        generator_a.trainable_variables)\n  generator_b_gradients = tape.gradient(total_gen_b_loss, \n                                        generator_b.trainable_variables)\n  \n  discriminator_x_gradients = tape.gradient(disc_x_loss, \n                                            discriminator_x.trainable_variables)\n  discriminator_y_gradients = tape.gradient(disc_y_loss, \n                                            discriminator_y.trainable_variables)\n  \n  # Apply the gradients to the optimizer\n  generator_a_optimizer.apply_gradients(zip(generator_a_gradients, \n                                            generator_a.trainable_variables))\n\n  generator_b_optimizer.apply_gradients(zip(generator_b_gradients, \n                                            generator_b.trainable_variables))\n  \n  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                discriminator_x.trainable_variables))\n  \n  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                discriminator_y.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.496204Z","iopub.execute_input":"2022-03-06T19:53:25.496868Z","iopub.status.idle":"2022-03-06T19:53:25.509259Z","shell.execute_reply.started":"2022-03-06T19:53:25.496828Z","shell.execute_reply":"2022-03-06T19:53:25.508578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n  start = time.time()\n\n  n = 0\n  for image_x, image_y in tf.data.Dataset.zip((train_normal, train_van)):\n    train_step(image_x, image_y)\n    if n % 10 == 0:\n      print ('.', end='')\n    n+=1\n\n  clear_output(wait=True)\n  # Using a consistent image (sample_horse) so that the progress of the model\n  # is clearly visible.\n  generate_images(generator_a, sample_normal)\n\n#   if (epoch + 1) % 5 == 0:\n#     ckpt_save_path = ckpt_manager.save()\n#     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n#                                                          ckpt_save_path))\n\n  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                      time.time()-start))","metadata":{"execution":{"iopub.status.busy":"2022-03-06T19:53:25.510692Z","iopub.execute_input":"2022-03-06T19:53:25.511183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for inp in test_normal.take(10):\n  generate_images(generator_a, inp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ngenerator_Save=generator_a.save('./cezanne.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_load = keras.models.load_model('./cezanne.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgurl=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/687px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg\"","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:47:09.535453Z","iopub.execute_input":"2022-03-07T04:47:09.535894Z","iopub.status.idle":"2022-03-07T04:47:09.548615Z","shell.execute_reply.started":"2022-03-07T04:47:09.535843Z","shell.execute_reply":"2022-03-07T04:47:09.547068Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from urllib.request import urlopen\nfrom PIL import Image\nimport cv2\nfrom numpy import asarray\nfrom scipy import misc\n\nimg = Image.open(urlopen(imgurl))\nbasewidth = 256\n#wpercent = (basewidth / float(img.size[0]))\nhsize = 256#int((float(img.size[1]) * float(wpercent)))\nimg = img.resize((basewidth, hsize))\n#img.save('resized_image.jpg')\n#img.show()\n#img = cv2.cvtColor(np.float32(img), cv2.COLOR_RGB2BGR)\n#image=misc.imread(img)\nimg=asarray(img)\nimg.astype('float32')\nimg=img/255.0\nimage_tensor = tf.convert_to_tensor(img, dtype=tf.float64)\nimage_tensor = tf.expand_dims(image_tensor, 0)\ngenerate_images(model_load, image_tensor)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:47:17.016447Z","iopub.execute_input":"2022-03-07T04:47:17.017063Z","iopub.status.idle":"2022-03-07T04:47:17.821678Z","shell.execute_reply.started":"2022-03-07T04:47:17.017026Z","shell.execute_reply":"2022-03-07T04:47:17.820234Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}